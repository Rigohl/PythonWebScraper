{
  "snippets": [
    {
      "id": "best-practices:monitoring",
      "category": "best-practices",
      "title": "Sistema Integral de Monitoreo",
      "content": "import asyncio\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\nimport logging\nimport json\nfrom prometheus_client import Counter, Gauge, Histogram\n\n@dataclass\nclass ScrapingMetrics:\n    success_rate: float\n    error_rate: float\n    avg_response_time: float\n    bandwidth_usage: float\n    cache_hit_rate: float\n\n@dataclass\nclass ResourceMetrics:\n    cpu_usage: float\n    memory_usage: float\n    disk_usage: float\n    network_io: float\n\nclass MonitoringSystem:\n    def __init__(self):\n        # Prometheus metrics\n        self.request_counter = Counter(\n            'scraper_requests_total',\n            'Total number of scraping requests',\n            ['domain', 'status']\n        )\n        self.response_time = Histogram(\n            'scraper_response_time_seconds',\n            'Response time in seconds',\n            ['domain']\n        )\n        self.error_gauge = Gauge(\n            'scraper_error_rate',\n            'Current error rate',\n            ['domain']\n        )\n        \n        self.metrics_history: Dict[str, List[ScrapingMetrics]] = {}\n        self.resource_history: List[ResourceMetrics] = []\n        self.alerts = []\n\n    async def collect_metrics(self, domain: str) -> ScrapingMetrics:\n        # Simular recolección de métricas\n        metrics = ScrapingMetrics(\n            success_rate=0.95,\n            error_rate=0.05,\n            avg_response_time=0.5,\n            bandwidth_usage=1024.0,\n            cache_hit_rate=0.8\n        )\n\n        if domain not in self.metrics_history:\n            self.metrics_history[domain] = []\n        \n        self.metrics_history[domain].append(metrics)\n        return metrics\n\n    async def monitor_resources(self) -> ResourceMetrics:\n        # Simular monitoreo de recursos\n        metrics = ResourceMetrics(\n            cpu_usage=50.0,\n            memory_usage=70.0,\n            disk_usage=30.0,\n            network_io=500.0\n        )\n\n        self.resource_history.append(metrics)\n        return metrics\n\n    def analyze_trends(self, domain: str, window: timedelta = timedelta(hours=1)):\n        if domain not in self.metrics_history:\n            return None\n\n        metrics = self.metrics_history[domain]\n        now = datetime.utcnow()\n\n        # Analizar tendencias en la ventana de tiempo\n        window_metrics = [\n            m for m in metrics[-100:]\n        ]\n\n        if not window_metrics:\n            return None\n\n        avg_success = sum(m.success_rate for m in window_metrics) / len(window_metrics)\n        avg_response = sum(m.avg_response_time for m in window_metrics) / len(window_metrics)\n\n        # Detectar anomalías\n        latest = metrics[-1]\n        if latest.error_rate > 0.2:\n            self.alerts.append({\n                'level': 'high',\n                'message': f'High error rate detected for {domain}',\n                'timestamp': now\n            })\n\n        if latest.avg_response_time > avg_response * 2:\n            self.alerts.append({\n                'level': 'medium',\n                'message': f'Response time degradation for {domain}',\n                'timestamp': now\n            })\n\n        return {\n            'current': latest,\n            'avg_success_rate': avg_success,\n            'avg_response_time': avg_response,\n            'trend': 'stable' if abs(latest.success_rate - avg_success) < 0.1 else\n                    'improving' if latest.success_rate > avg_success else 'degrading'\n        }\n\n    def get_health_report(self) -> Dict:\n        report = {\n            'overall_health': 'green',\n            'domains': {},\n            'resources': {\n                'status': 'normal',\n                'warnings': []\n            },\n            'alerts': self.alerts[-5:]\n        }\n\n        # Analizar salud por dominio\n        for domain in self.metrics_history:\n            trend = self.analyze_trends(domain)\n            if trend:\n                report['domains'][domain] = {\n                    'health': 'green' if trend['avg_success_rate'] > 0.9 else\n                             'yellow' if trend['avg_success_rate'] > 0.7 else 'red',\n                    'trend': trend['trend']\n                }\n\n        # Analizar recursos\n        if self.resource_history:\n            latest = self.resource_history[-1]\n            if latest.cpu_usage > 80:\n                report['resources']['warnings'].append('High CPU usage')\n            if latest.memory_usage > 85:\n                report['resources']['warnings'].append('High memory usage')\n\n            if report['resources']['warnings']:\n                report['resources']['status'] = 'warning'\n\n        return report\n\n    async def run_monitoring_loop(self):\n        while True:\n            try:\n                # Monitorear recursos\n                await self.monitor_resources()\n\n                # Monitorear cada dominio\n                for domain in self.metrics_history:\n                    await self.collect_metrics(domain)\n\n                # Generar reporte\n                report = self.get_health_report()\n                logging.info(f\"Health Report: {json.dumps(report, indent=2)}\")\n\n                # Limpiar historia antigua\n                now = datetime.utcnow()\n                old = now - timedelta(days=7)\n\n                for domain in self.metrics_history:\n                    self.metrics_history[domain] = [\n                        m for m in self.metrics_history[domain][-1000:]\n                    ]\n\n                self.resource_history = self.resource_history[-1000:]\n                self.alerts = [a for a in self.alerts if a['timestamp'] > old]\n\n            except Exception as e:\n                logging.error(f\"Error in monitoring loop: {e}\")\n\n            await asyncio.sleep(60)  # Esperar 1 minuto\n\n# Ejemplo de uso\nasync def main():\n    monitor = MonitoringSystem()\n    \n    # Iniciar loop de monitoreo en background\n    monitoring_task = asyncio.create_task(\n        monitor.run_monitoring_loop()\n    )\n\n    try:\n        # Simular algunas métricas\n        domains = ['example.com', 'test.org']\n        for domain in domains:\n            await monitor.collect_metrics(domain)\n\n        # Obtener reporte después de algunas iteraciones\n        await asyncio.sleep(5)\n        report = monitor.get_health_report()\n        print(json.dumps(report, indent=2))\n\n    finally:\n        monitoring_task.cancel()\n        try:\n            await monitoring_task\n        except asyncio.CancelledError:\n            pass\n\nif __name__ == '__main__':\n    asyncio.run(main())",
      "tags": [
        "monitoring",
        "metrics",
        "prometheus",
        "health-check"
      ],
      "quality_score": 0.95,
      "added_ts": 1757065409.542256
    },
    {
      "id": "best-practices:code-quality",
      "category": "best-practices",
      "title": "Prácticas de Calidad de Código",
      "content": "# 1. Logging y Documentación\n\nfrom typing import Optional, Dict, Any\nimport logging\nfrom functools import wraps\nimport inspect\n\ndef setup_logging(level: int = logging.INFO):\n    \"\"\"Configura logging estructurado con formateo JSON\"\"\"\n    logging.basicConfig(\n        level=level,\n        format='%(asctime)s [%(levelname)s] %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S'\n    )\n\n# 2. Decoradores para documentación y contratos\ndef validate_params(func):\n    \"\"\"Valida tipos de parámetros usando type hints\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        sig = inspect.signature(func)\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n        \n        for param_name, value in bound_args.arguments.items():\n            param = sig.parameters[param_name]\n            if param.annotation != inspect.Parameter.empty:\n                if not isinstance(value, param.annotation):\n                    raise TypeError(\n                        f\"Parameter {param_name} must be {param.annotation.__name__}\"\n                    )\n        \n        return func(*args, **kwargs)\n    return wrapper\n\ndef log_calls(func):\n    \"\"\"Registra llamadas a funciones con parámetros y resultados\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        func_name = func.__name__\n        params = {\n            'args': args,\n            'kwargs': kwargs\n        }\n        \n        logging.debug(f\"Calling {func_name} with params: {params}\")\n        try:\n            result = func(*args, **kwargs)\n            logging.debug(f\"{func_name} returned: {result}\")\n            return result\n        except Exception as e:\n            logging.error(\n                f\"Error in {func_name}: {str(e)}\",\n                exc_info=True\n            )\n            raise\n    return wrapper\n\n# 3. Clases base con contratos claros\nclass BaseWebScraper:\n    \"\"\"Clase base para scrapers con contrato definido\"\"\"\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        self.config = config or {}\n        self._validate_config()\n        self._setup()\n    \n    def _validate_config(self) -> None:\n        \"\"\"Valida la configuración requerida\"\"\"\n        required = {'url', 'selectors'}\n        missing = required - set(self.config)\n        if missing:\n            raise ValueError(\n                f\"Missing required config: {missing}\"\n            )\n    \n    def _setup(self) -> None:\n        \"\"\"Inicializa recursos necesarios\"\"\"\n        pass\n    \n    def cleanup(self) -> None:\n        \"\"\"Libera recursos\"\"\"\n        pass\n    \n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.cleanup()\n\n# 4. Testing Fixtures y Helpers\nimport pytest\nfrom unittest.mock import Mock, patch\n\n@pytest.fixture\ndef mock_scraper():\n    \"\"\"Fixture para testing con mock de scraper\"\"\"\n    scraper = Mock(spec=BaseWebScraper)\n    scraper.config = {\n        'url': 'http://test.com',\n        'selectors': {'title': 'h1'}\n    }\n    return scraper\n\n# 5. Patrón Repository para acceso a datos\nclass BaseRepository:\n    \"\"\"Capa de abstracción para persistencia\"\"\"\n    def __init__(self, connection_string: str):\n        self.connection_string = connection_string\n    \n    async def get(self, id_: str) -> Optional[Dict]:\n        \"\"\"Recupera un registro por ID\"\"\"\n        raise NotImplementedError\n    \n    async def save(self, data: Dict) -> str:\n        \"\"\"Guarda un nuevo registro\"\"\"\n        raise NotImplementedError\n    \n    async def update(self, id_: str, data: Dict) -> bool:\n        \"\"\"Actualiza un registro existente\"\"\"\n        raise NotImplementedError\n    \n    async def delete(self, id_: str) -> bool:\n        \"\"\"Elimina un registro\"\"\"\n        raise NotImplementedError\n\n# 6. Manejo de configuración\nimport yaml\nfrom pathlib import Path\n\ndef load_config(config_path: Path) -> Dict:\n    \"\"\"Carga configuración desde YAML con validación\"\"\"\n    if not config_path.exists():\n        raise FileNotFoundError(\n            f\"Config file not found: {config_path}\"\n        )\n    \n    with config_path.open() as f:\n        config = yaml.safe_load(f)\n    \n    if not isinstance(config, dict):\n        raise ValueError(\"Config must be a dictionary\")\n    \n    return config\n\n# 7. Ejemplo de uso integrado\n@validate_params\n@log_calls\nclass ProductScraper(BaseWebScraper):\n    \"\"\"Scraper específico con buenas prácticas\"\"\"\n    def __init__(\n        self,\n        config: Dict[str, Any],\n        repository: BaseRepository\n    ):\n        super().__init__(config)\n        self.repository = repository\n    \n    async def scrape_product(\n        self,\n        url: str\n    ) -> Optional[Dict[str, Any]]:\n        \"\"\"Extrae información de un producto\"\"\"\n        try:\n            # Implementación\n            product_data = {'url': url}\n            \n            # Persistir datos\n            await self.repository.save(product_data)\n            \n            return product_data\n        \n        except Exception as e:\n            logging.error(\n                f\"Error scraping product {url}: {e}\",\n                exc_info=True\n            )\n            return None\n\n# 8. Tests\ndef test_product_scraper(mock_scraper):\n    \"\"\"Test unitario de ejemplo\"\"\"\n    with patch('aiohttp.ClientSession') as mock_session:\n        mock_response = Mock()\n        mock_response.text.return_value = '<html></html>'\n        mock_session.get.return_value.__aenter__.return_value = mock_response\n        \n        scraper = mock_scraper\n        result = await scraper.scrape_product('http://test.com')\n        \n        assert result is not None\n        assert 'url' in result",
      "tags": [
        "code-quality",
        "testing",
        "logging",
        "best-practices"
      ],
      "quality_score": 0.96,
      "added_ts": 1757065409.542256
    },
    {
      "id": "best-practices:proxy-rotation",
      "category": "best-practices",
      "title": "Sistema Avanzado de Gestión de Proxies",
      "content": "from typing import List, Dict, Optional\nimport aiohttp\nimport asyncio\from datetime import datetime, timedelta\nimport random\nimport logging\n\nclass ProxyManager:\n    def __init__(self):\n        self.proxies: List[Dict] = []\n        self.proxy_stats: Dict[str, Dict] = {}\n        self.min_success_rate = 0.7\n        self.max_errors = 5\n        self.cooldown_period = timedelta(minutes=10)\n\n    async def add_proxy(self, proxy: Dict):\n        \"\"\"Añade un nuevo proxy al pool\"\"\"\n        proxy_id = proxy['host'] + ':' + str(proxy['port'])\n        self.proxies.append(proxy)\n        self.proxy_stats[proxy_id] = {\n            'success': 0,\n            'failures': 0,\n            'last_used': None,\n            'last_error': None,\n            'cooldown_until': None\n        }\n\n    async def test_proxy(self, proxy: Dict) -> bool:\n        \"\"\"Prueba un proxy contra múltiples servicios\"\"\"\n        test_urls = [\n            'http://httpbin.org/ip',\n            'https://api.ipify.org?format=json'\n        ]\n        \n        async with aiohttp.ClientSession() as session:\n            for url in test_urls:\n                try:\n                    async with session.get(\n                        url,\n                        proxy=f\"http://{proxy['host']}:{proxy['port']}\",\n                        timeout=10\n                    ) as response:\n                        if response.status == 200:\n                            return True\n                except Exception:\n                    continue\n        return False\n\n    def _calculate_proxy_score(self, stats: Dict) -> float:\n        \"\"\"Calcula un puntaje para el proxy basado en su rendimiento\"\"\"\n        total = stats['success'] + stats['failures']\n        if total == 0:\n            return 1.0  # Nuevos proxies obtienen máxima prioridad\n\n        success_rate = stats['success'] / total\n        time_factor = 1.0\n\n        if stats['last_error']:\n            hours_since_error = (\n                datetime.now() - stats['last_error']\n            ).total_seconds() / 3600\n            time_factor = min(1.0, hours_since_error / 24)\n\n        return success_rate * time_factor\n\n    async def get_proxy(self) -> Optional[Dict]:\n        \"\"\"Obtiene el mejor proxy disponible\"\"\"\n        now = datetime.now()\n        available_proxies = [\n            p for p in self.proxies\n            if not self.proxy_stats[p['host'] + ':' + str(p['port'])]['cooldown_until']\n            or self.proxy_stats[p['host'] + ':' + str(p['port'])]['cooldown_until'] < now\n        ]\n\n        if not available_proxies:\n            return None\n\n        # Ordenar por puntaje\n        scored_proxies = [\n            (p, self._calculate_proxy_score(\n                self.proxy_stats[p['host'] + ':' + str(p['port'])]\n            ))\n            for p in available_proxies\n        ]\n        scored_proxies.sort(key=lambda x: x[1], reverse=True)\n\n        # Selección ponderada aleatoria\n        total_score = sum(score for _, score in scored_proxies)\n        if total_score == 0:\n            return random.choice(available_proxies)\n\n        threshold = random.uniform(0, total_score)\n        current_sum = 0\n        for proxy, score in scored_proxies:\n            current_sum += score\n            if current_sum >= threshold:\n                return proxy\n\n        return scored_proxies[0][0]  # Fallback al mejor proxy\n\n    async def report_result(self, proxy: Dict, success: bool, error: Optional[str] = None):\n        \"\"\"Reporta el resultado de usar un proxy\"\"\"\n        proxy_id = proxy['host'] + ':' + str(proxy['port'])\n        stats = self.proxy_stats[proxy_id]\n\n        if success:\n            stats['success'] += 1\n            stats['last_used'] = datetime.now()\n        else:\n            stats['failures'] += 1\n            stats['last_error'] = datetime.now()\n            if error:\n                stats['last_error_message'] = error\n\n            # Verificar si necesita cooldown\n            if stats['failures'] >= self.max_errors:\n                stats['cooldown_until'] = datetime.now() + self.cooldown_period\n\n    async def cleanup(self):\n        \"\"\"Limpia proxies con mal rendimiento\"\"\"\n        to_remove = []\n        for proxy in self.proxies:\n            proxy_id = proxy['host'] + ':' + str(proxy['port'])\n            stats = self.proxy_stats[proxy_id]\n            total = stats['success'] + stats['failures']\n\n            if total >= 10:  # Mínimo de intentos para evaluar\n                success_rate = stats['success'] / total\n                if success_rate < self.min_success_rate:\n                    to_remove.append(proxy)\n\n        for proxy in to_remove:\n            self.proxies.remove(proxy)\n            proxy_id = proxy['host'] + ':' + str(proxy['port'])\n            del self.proxy_stats[proxy_id]\n\n    async def rotate_proxies(self):\n        \"\"\"Mantiene el pool de proxies actualizado\"\"\"\n        while True:\n            try:\n                await self.cleanup()\n                \n                # Verificar si necesitamos más proxies\n                if len(self.proxies) < 10:\n                    new_proxies = await self.fetch_new_proxies()\n                    for proxy in new_proxies:\n                        if await self.test_proxy(proxy):\n                            await self.add_proxy(proxy)\n\n            except Exception as e:\n                logging.error(f\"Error en rotación de proxies: {e}\")\n\n            await asyncio.sleep(300)  # Cada 5 minutos\n\n    async def fetch_new_proxies(self) -> List[Dict]:\n        \"\"\"Obtiene nuevos proxies de múltiples fuentes\"\"\"\n        # Implementar lógica para obtener proxies de diferentes proveedores\n        return []",
      "tags": [
        "proxies",
        "rotation",
        "management",
        "best-practices"
      ],
      "quality_score": 0.95,
      "added_ts": 1757065409.542256
    },
    {
      "id": "best-practices:session-management",
      "category": "best-practices",
      "title": "Gestión Avanzada de Sesiones",
      "content": "import asyncio\nfrom typing import Dict, Optional\nfrom datetime import datetime, timedelta\nimport logging\nimport aiohttp\nimport json\nfrom dataclasses import dataclass\nfrom fake_useragent import UserAgent\nfrom bs4 import BeautifulSoup\nimport re\n\n@dataclass\nclass SessionConfig:\n    cookies_enabled: bool = True\n    max_retries: int = 3\n    timeout: int = 30\n    respect_robots: bool = True\n    user_agent_rotation: bool = True\n    session_lifetime: timedelta = timedelta(hours=1)\n\nclass SessionManager:\n    def __init__(self, config: SessionConfig):\n        self.config = config\n        self.sessions: Dict[str, aiohttp.ClientSession] = {}\n        self.session_stats: Dict[str, Dict] = {}\n        self.user_agent = UserAgent()\n        self.robots_cache: Dict[str, Dict] = {}\n\n    async def get_session(self, domain: str) -> aiohttp.ClientSession:\n        \"\"\"Obtiene o crea una sesión para el dominio\"\"\"\n        now = datetime.now()\n        \n        # Verificar si la sesión existe y es válida\n        if domain in self.sessions:\n            stats = self.session_stats[domain]\n            if (now - stats['created_at']) < self.config.session_lifetime:\n                return self.sessions[domain]\n            else:\n                await self.close_session(domain)\n\n        # Crear nueva sesión\n        headers = self._generate_headers(domain)\n        cookies = await self._load_cookies(domain)\n        \n        session = aiohttp.ClientSession(\n            headers=headers,\n            cookies=cookies,\n            timeout=aiohttp.ClientTimeout(total=self.config.timeout)\n        )\n\n        self.sessions[domain] = session\n        self.session_stats[domain] = {\n            'created_at': now,\n            'requests': 0,\n            'errors': 0,\n            'last_request': None\n        }\n\n        return session\n\n    def _generate_headers(self, domain: str) -> Dict:\n        \"\"\"Genera headers realistas para la sesión\"\"\"\n        headers = {\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n            'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',\n            'Accept-Encoding': 'gzip, deflate, br',\n            'DNT': '1',\n            'Connection': 'keep-alive',\n            'Upgrade-Insecure-Requests': '1',\n            'Sec-Fetch-Dest': 'document',\n            'Sec-Fetch-Mode': 'navigate',\n            'Sec-Fetch-Site': 'none',\n            'Sec-Fetch-User': '?1',\n            'Cache-Control': 'max-age=0'\n        }\n\n        if self.config.user_agent_rotation:\n            headers['User-Agent'] = self.user_agent.random\n\n        return headers\n\n    async def _load_cookies(self, domain: str) -> Dict:\n        \"\"\"Carga cookies guardadas para el dominio\"\"\"\n        if not self.config.cookies_enabled:\n            return {}\n\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.get(f'https://{domain}') as response:\n                    return {k: v.value for k, v in response.cookies.items()}\n        except Exception:\n            return {}\n\n    async def close_session(self, domain: str):\n        \"\"\"Cierra y limpia una sesión\"\"\"\n        if domain in self.sessions:\n            await self.sessions[domain].close()\n            del self.sessions[domain]\n            del self.session_stats[domain]\n\n    async def can_scrape(self, domain: str, path: str) -> bool:\n        \"\"\"Verifica robots.txt antes de scraping\"\"\"\n        if not self.config.respect_robots:\n            return True\n\n        # Verificar caché de robots.txt\n        if domain not in self.robots_cache:\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.get(f'https://{domain}/robots.txt') as response:\n                        if response.status == 200:\n                            text = await response.text()\n                            self.robots_cache[domain] = self._parse_robots_txt(text)\n                        else:\n                            self.robots_cache[domain] = {'allow': [], 'disallow': []}\n            except Exception:\n                self.robots_cache[domain] = {'allow': [], 'disallow': []}\n\n        rules = self.robots_cache[domain]\n        \n        # Verificar reglas\n        for pattern in rules['disallow']:\n            if re.match(pattern, path):\n                return False\n\n        return True\n\n    def _parse_robots_txt(self, text: str) -> Dict:\n        \"\"\"Parsea robots.txt en reglas\"\"\"\n        rules = {'allow': [], 'disallow': []}\n        lines = text.split('\\n')\n        is_relevant_agent = False\n\n        for line in lines:\n            line = line.strip().lower()\n            if not line or line.startswith('#'):\n                continue\n\n            if line.startswith('user-agent:'):\n                agent = line.split(':', 1)[1].strip()\n                is_relevant_agent = agent in ['*', 'pythonrequests', 'python']\n                continue\n\n            if is_relevant_agent:\n                if line.startswith('allow:'):\n                    path = line.split(':', 1)[1].strip()\n                    rules['allow'].append(self._path_to_regex(path))\n                elif line.startswith('disallow:'):\n                    path = line.split(':', 1)[1].strip()\n                    rules['disallow'].append(self._path_to_regex(path))\n\n        return rules\n\n    def _path_to_regex(self, path: str) -> str:\n        \"\"\"Convierte patrón de robots.txt a regex\"\"\"\n        return (\n            path.replace('.', '\\.')\n            .replace('*', '.*')\n            .replace('?', '\\?')\n            + '$'\n        )\n\n    async def update_stats(self, domain: str, success: bool):\n        \"\"\"Actualiza estadísticas de la sesión\"\"\"\n        if domain in self.session_stats:\n            stats = self.session_stats[domain]\n            stats['requests'] += 1\n            if not success:\n                stats['errors'] += 1\n            stats['last_request'] = datetime.now()\n\n            # Verificar si necesitamos rotar la sesión\n            if (\n                stats['errors'] >= self.config.max_retries or\n                stats['requests'] >= 1000  # Límite arbitrario\n            ):\n                await self.close_session(domain)\n\n    async def cleanup(self):\n        \"\"\"Limpia sesiones expiradas\"\"\"\n        now = datetime.now()\n        domains_to_close = [\n            domain for domain, stats in self.session_stats.items()\n            if (now - stats['created_at']) >= self.config.session_lifetime\n        ]\n\n        for domain in domains_to_close:\n            await self.close_session(domain)",
      "tags": [
        "session-management",
        "cookies",
        "headers",
        "robots-txt"
      ],
      "quality_score": 0.96,
      "added_ts": 1757065409.542256
    },
    {
      "id": "best-practices:security",
      "category": "best-practices",
      "title": "Prácticas de Seguridad para Web Scraping",
      "content": "import hashlib\nimport hmac\nimport base64\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass\nimport logging\nimport json\nimport re\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass SecurityConfig:\n    rate_limiting: bool = True\n    requests_per_minute: int = 60\n    max_concurrent: int = 10\n    encryption_key: str = ''\n    verify_ssl: bool = True\n    allowed_domains: List[str] = None\n    blocked_patterns: List[str] = None\n\nclass SecurityManager:\n    def __init__(self, config: SecurityConfig):\n        self.config = config\n        self.request_history: Dict[str, List[datetime]] = {}\n        self.blocked_ips: Dict[str, datetime] = {}\n        self.suspicious_patterns: List[re.Pattern] = [\n            re.compile(pattern) for pattern in [\n                r'<script>.*?</script>',\n                r'eval\\(',\n                r'document\\.cookie',\n                r'window\\.location',\n                r'<iframe',\n                r'data:text/html'\n            ]\n        ]\n\n    def can_make_request(self, domain: str) -> bool:\n        \"\"\"Verifica límites de velocidad\"\"\"\n        if not self.config.rate_limiting:\n            return True\n\n        now = datetime.now()\n        if domain not in self.request_history:\n            self.request_history[domain] = []\n\n        # Limpiar historial antiguo\n        self.request_history[domain] = [\n            ts for ts in self.request_history[domain]\n            if (now - ts) < timedelta(minutes=1)\n        ]\n\n        return len(self.request_history[domain]) < self.config.requests_per_minute\n\n    def log_request(self, domain: str):\n        \"\"\"Registra una nueva solicitud\"\"\"\n        if domain in self.request_history:\n            self.request_history[domain].append(datetime.now())\n\n    def is_domain_allowed(self, domain: str) -> bool:\n        \"\"\"Verifica si el dominio está permitido\"\"\"\n        if not self.config.allowed_domains:\n            return True\n        return domain in self.config.allowed_domains\n\n    def is_content_safe(self, content: str) -> bool:\n        \"\"\"Verifica contenido sospechoso\"\"\"\n        for pattern in self.suspicious_patterns:\n            if pattern.search(content):\n                return False\n        return True\n\n    def encrypt_data(self, data: Dict) -> str:\n        \"\"\"Encripta datos sensibles\"\"\"\n        if not self.config.encryption_key:\n            return json.dumps(data)\n\n        key = self.config.encryption_key.encode()\n        message = json.dumps(data).encode()\n\n        h = hmac.new(key, message, hashlib.sha256)\n        signature = base64.b64encode(h.digest()).decode()\n\n        encrypted = {\n            'data': base64.b64encode(message).decode(),\n            'signature': signature,\n            'timestamp': datetime.now().isoformat()\n        }\n\n        return json.dumps(encrypted)\n\n    def decrypt_data(self, encrypted_str: str) -> Optional[Dict]:\n        \"\"\"Desencripta datos\"\"\"\n        try:\n            encrypted = json.loads(encrypted_str)\n            if not all(k in encrypted for k in ['data', 'signature', 'timestamp']):\n                return None\n\n            # Verificar firma\n            key = self.config.encryption_key.encode()\n            message = base64.b64decode(encrypted['data'])\n            h = hmac.new(key, message, hashlib.sha256)\n            expected_signature = base64.b64encode(h.digest()).decode()\n\n            if not hmac.compare_digest(\n                encrypted['signature'],\n                expected_signature\n            ):\n                return None\n\n            # Verificar timestamp\n            ts = datetime.fromisoformat(encrypted['timestamp'])\n            if datetime.now() - ts > timedelta(hours=24):\n                return None\n\n            return json.loads(message.decode())\n\n        except Exception as e:\n            logging.error(f\"Error decrypting data: {e}\")\n            return None\n\n    def sanitize_input(self, text: str) -> str:\n        \"\"\"Sanitiza entrada de usuario\"\"\"\n        # Eliminar scripts y otros elementos peligrosos\n        text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.I | re.S)\n        text = re.sub(r'<iframe.*?>.*?</iframe>', '', text, flags=re.I | re.S)\n        text = re.sub(r'on\\w+=\".*?\"', '', text, flags=re.I)\n        \n        # Escapar caracteres especiales\n        text = text.replace('<', '&lt;')\n        text = text.replace('>', '&gt;')\n        text = text.replace('\"', '&quot;')\n        text = text.replace(\"'\", '&#x27;')\n        \n        return text.strip()\n\n    def is_blocked_pattern(self, url: str) -> bool:\n        \"\"\"Verifica patrones bloqueados en URLs\"\"\"\n        if not self.config.blocked_patterns:\n            return False\n\n        return any(\n            re.search(pattern, url)\n            for pattern in self.config.blocked_patterns\n        )\n\n    def validate_response(self, headers: Dict, content: str) -> bool:\n        \"\"\"Valida respuesta HTTP\"\"\"\n        # Verificar headers sospechosos\n        suspicious_headers = {\n            'X-Frame-Options': 'DENY',\n            'Content-Security-Policy': 'frame-ancestors\\'none\\''\n        }\n\n        for header, expected in suspicious_headers.items():\n            if header in headers and headers[header] != expected:\n                return False\n\n        # Verificar contenido malicioso\n        return self.is_content_safe(content)",
      "tags": [
        "security",
        "encryption",
        "validation",
        "rate-limiting"
      ],
      "quality_score": 0.97,
      "added_ts": 1757065409.542256
    },
    {
      "id": "best-practices:integrated-system",
      "category": "best-practices",
      "title": "Sistema Integral de Web Scraping",
      "content": "from typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\nimport asyncio\nimport logging\nfrom datetime import datetime, timedelta\nimport aiohttp\nfrom bs4 import BeautifulSoup\nfrom prometheus_client import Counter, Gauge, Histogram\nimport json\n\n@dataclass\nclass ScraperConfig:\n    # Configuración de seguridad\n    rate_limiting: bool = True\n    requests_per_minute: int = 60\n    max_concurrent: int = 10\n    verify_ssl: bool = True\n    allowed_domains: List[str] = None\n    encryption_key: str = ''\n\n    # Configuración de sesiones\n    cookies_enabled: bool = True\n    user_agent_rotation: bool = True\n    session_lifetime: timedelta = timedelta(hours=1)\n    respect_robots: bool = True\n\n    # Configuración de proxies\n    use_proxies: bool = True\n    min_proxy_score: float = 0.7\n    proxy_cooldown: timedelta = timedelta(minutes=10)\n\n    # Configuración de monitoreo\n    enable_monitoring: bool = True\n    metrics_port: int = 9090\n    alert_threshold: float = 0.8\n\nclass IntegratedScraper:\n    def __init__(self, config: ScraperConfig):\n        self.config = config\n        \n        # Inicializar componentes\n        self.security = SecurityManager(SecurityConfig(\n            rate_limiting=config.rate_limiting,\n            requests_per_minute=config.requests_per_minute,\n            max_concurrent=config.max_concurrent,\n            encryption_key=config.encryption_key,\n            verify_ssl=config.verify_ssl,\n            allowed_domains=config.allowed_domains\n        ))\n\n        self.session_manager = SessionManager(SessionConfig(\n            cookies_enabled=config.cookies_enabled,\n            user_agent_rotation=config.user_agent_rotation,\n            session_lifetime=config.session_lifetime,\n            respect_robots=config.respect_robots\n        ))\n\n        if config.use_proxies:\n            self.proxy_manager = ProxyManager()\n        else:\n            self.proxy_manager = None\n\n        if config.enable_monitoring:\n            self.monitoring = MonitoringSystem()\n        else:\n            self.monitoring = None\n\n        # Métricas\n        self.request_counter = Counter(\n            'scraper_requests_total',\n            'Total number of requests',\n            ['domain', 'status']\n        )\n        self.response_time = Histogram(\n            'scraper_response_time_seconds',\n            'Response time in seconds',\n            ['domain']\n        )\n\n    async def initialize(self):\n        \"\"\"Inicializa el sistema\"\"\"\n        if self.proxy_manager:\n            # Iniciar rotación de proxies en background\n            self.proxy_rotation_task = asyncio.create_task(\n                self.proxy_manager.rotate_proxies()\n            )\n\n        if self.monitoring:\n            # Iniciar monitoreo en background\n            self.monitoring_task = asyncio.create_task(\n                self.monitoring.run_monitoring_loop()\n            )\n\n    async def scrape(self, url: str, **kwargs) -> Optional[Dict]:\n        \"\"\"Método principal de scraping con todas las protecciones\"\"\"\n        domain = self._extract_domain(url)\n\n        # Verificaciones de seguridad\n        if not self.security.is_domain_allowed(domain):\n            logging.warning(f\"Domain not allowed: {domain}\")\n            return None\n\n        if not self.security.can_make_request(domain):\n            logging.warning(f\"Rate limit exceeded for {domain}\")\n            return None\n\n        if self.monitoring:\n            start_time = datetime.now()\n\n        try:\n            # Obtener sesión y proxy\n            session = await self.session_manager.get_session(domain)\n            proxy = await self.proxy_manager.get_proxy() if self.proxy_manager else None\n\n            # Verificar robots.txt\n            if not await self.session_manager.can_scrape(domain, self._get_path(url)):\n                logging.info(f\"URL not allowed by robots.txt: {url}\")\n                return None\n\n            # Realizar request con reintentos\n            data = await self._make_request_with_retry(\n                session, url, proxy, **kwargs\n            )\n\n            if data and self.security.is_content_safe(data):\n                # Procesar y sanitizar datos\n                result = self._process_response(data)\n                sanitized = self.security.sanitize_input(str(result))\n\n                # Encriptar si es necesario\n                if self.config.encryption_key:\n                    return self.security.encrypt_data(sanitized)\n                return sanitized\n\n        except Exception as e:\n            logging.error(f\"Error scraping {url}: {e}\")\n            if self.monitoring:\n                self.request_counter.labels(\n                    domain=domain,\n                    status='error'\n                ).inc()\n            return None\n\n        finally:\n            # Actualizar métricas y estadísticas\n            if self.monitoring:\n                duration = (datetime.now() - start_time).total_seconds()\n                self.response_time.labels(domain=domain).observe(duration)\n\n            self.security.log_request(domain)\n\n    async def _make_request_with_retry(\n        self,\n        session: aiohttp.ClientSession,\n        url: str,\n        proxy: Optional[Dict],\n        max_retries: int = 3,\n        **kwargs\n    ) -> Optional[str]:\n        \"\"\"Realiza request con reintentos automáticos\"\"\"\n        retries = 0\n        while retries < max_retries:\n            try:\n                proxy_url = f\"http://{proxy['host']}:{proxy['port']}\" if proxy else None\n                async with session.get(\n                    url,\n                    proxy=proxy_url,\n                    ssl=self.config.verify_ssl,\n                    **kwargs\n                ) as response:\n                    if response.status == 200:\n                        if proxy:\n                            await self.proxy_manager.report_result(proxy, True)\n                        return await response.text()\n\n                    # Manejar otros códigos de estado\n                    if response.status in {403, 429, 503}:\n                        if proxy:\n                            await self.proxy_manager.report_result(\n                                proxy,\n                                False,\n                                f\"Status {response.status}\"\n                            )\n                        await asyncio.sleep(2 ** retries)\n                    else:\n                        logging.warning(\n                            f\"Unexpected status {response.status} for {url}\"\n                        )\n                        return None\n\n            except Exception as e:\n                if proxy:\n                    await self.proxy_manager.report_result(\n                        proxy,\n                        False,\n                        str(e)\n                    )\n                logging.warning(f\"Error on try {retries + 1}: {e}\")\n                await asyncio.sleep(2 ** retries)\n\n            retries += 1\n\n        return None\n\n    def _process_response(self, html: str) -> Dict:\n        \"\"\"Procesa respuesta HTML en datos estructurados\"\"\"\n        soup = BeautifulSoup(html, 'html.parser')\n        \n        # Implementar lógica de extracción específica\n        return {}\n\n    def _extract_domain(self, url: str) -> str:\n        \"\"\"Extrae dominio de URL\"\"\"\n        from urllib.parse import urlparse\n        return urlparse(url).netloc\n\n    def _get_path(self, url: str) -> str:\n        \"\"\"Extrae path de URL\"\"\"\n        from urllib.parse import urlparse\n        return urlparse(url).path\n\n    async def cleanup(self):\n        \"\"\"Limpia recursos\"\"\"\n        # Cancelar tareas background\n        if hasattr(self, 'proxy_rotation_task'):\n            self.proxy_rotation_task.cancel()\n        if hasattr(self, 'monitoring_task'):\n            self.monitoring_task.cancel()\n\n        # Limpiar sesiones\n        domains = list(self.session_manager.sessions.keys())\n        for domain in domains:\n            await self.session_manager.close_session(domain)\n\n        # Esperar que se completen las tareas\n        pending = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]\n        await asyncio.gather(*pending, return_exceptions=True)\n\n# Ejemplo de uso\nasync def main():\n    config = ScraperConfig(\n        allowed_domains=['example.com'],\n        encryption_key='secret-key',\n        use_proxies=True,\n        enable_monitoring=True\n    )\n\n    scraper = IntegratedScraper(config)\n    await scraper.initialize()\n\n    try:\n        result = await scraper.scrape('https://example.com')\n        if result:\n            print(json.dumps(result, indent=2))\n    finally:\n        await scraper.cleanup()\n\nif __name__ == '__main__':\n    asyncio.run(main())",
      "tags": [
        "integrated-system",
        "best-practices",
        "scraping",
        "security",
        "monitoring"
      ],
      "quality_score": 0.98,
      "added_ts": 1757065409.542256
    },
    {
      "id": "errors:resilient-scraping",
      "category": "errors",
      "title": "Sistema Resiliente de Scraping",
      "content": "import asyncio\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\nimport aiohttp\nimport logging\nimport json\nfrom enum import Enum\n\n# Definiciones base\nclass ErrorCategory(Enum):\n    NETWORK = 'network'\n    HTTP = 'http'\n    PARSER = 'parser'\n    THROTTLING = 'throttling'\n    AUTHENTICATION = 'auth'\n    STRUCTURAL = 'structural'\n    UNKNOWN = 'unknown'\n\n@dataclass\nclass ErrorSignature:\n    category: ErrorCategory\n    error_type: str\n    message: str\n    context: Dict\n    timestamp: datetime\n\nclass RetryStrategy:\n    def __init__(self):\n        self.base_delay = 1.0\n        self.max_delay = 300.0  # 5 minutes\n        self.max_retries = 5\n        self.jitter = 0.1\n\n    def get_delay(self, attempt: int, error: ErrorSignature) -> float:\n        # Delay base exponencial\n        delay = min(\n            self.base_delay * (2 ** attempt),\n            self.max_delay\n        )\n\n        # Ajustar según categoría\n        if error.category == ErrorCategory.THROTTLING:\n            delay *= 2\n        elif error.category == ErrorCategory.NETWORK:\n            delay *= 1.5\n\n        # Añadir jitter\n        delay *= (1 + (random.random() - 0.5) * self.jitter)\n\n        return delay\n\n# Sistema de detección y categorización\nclass ErrorDetector:\n    def __init__(self):\n        self.patterns = {\n            ErrorCategory.NETWORK: [\n                r'ConnectionError',\n                r'TimeoutError',\n                r'SSLError'\n            ],\n            ErrorCategory.HTTP: [\n                r'4\\d\\d',\n                r'5\\d\\d'\n            ],\n            ErrorCategory.PARSER: [\n                r'ParserError',\n                r'ValueError: Invalid HTML'\n            ],\n            ErrorCategory.THROTTLING: [\n                r'429',\n                r'too many requests',\n                r'rate limit'\n            ],\n            ErrorCategory.AUTHENTICATION: [\n                r'401',\n                r'403',\n                r'invalid token'\n            ],\n            ErrorCategory.STRUCTURAL: [\n                r'NoSuchElementException',\n                r'ElementNotFound',\n                r'InvalidSelector'\n            ]\n        }\n\n    def categorize(self, error: Exception, context: Dict) -> ErrorSignature:\n        error_str = str(error).lower()\n        \n        for category, patterns in self.patterns.items():\n            if any(re.search(p, error_str, re.I) for p in patterns):\n                return ErrorSignature(\n                    category=category,\n                    error_type=type(error).__name__,\n                    message=str(error),\n                    context=context,\n                    timestamp=datetime.utcnow()\n                )\n\n        return ErrorSignature(\n            category=ErrorCategory.UNKNOWN,\n            error_type=type(error).__name__,\n            message=str(error),\n            context=context,\n            timestamp=datetime.utcnow()\n        )\n\n# Sistema de recuperación y resilencia\nclass ResilienceManager:\n    def __init__(self):\n        self.error_detector = ErrorDetector()\n        self.retry_strategy = RetryStrategy()\n        self.error_history: List[ErrorSignature] = []\n        self.domain_stats: Dict[str, Dict] = {}\n\n    async def execute_with_resilience(\n        self,\n        coroutine,\n        context: Dict,\n        max_retries: Optional[int] = None\n    ):\n        attempt = 0\n        domain = context.get('domain', 'unknown')\n\n        while True:\n            try:\n                result = await coroutine\n                \n                # Actualizar estadísticas de éxito\n                if domain not in self.domain_stats:\n                    self.domain_stats[domain] = {\n                        'success': 0,\n                        'errors': 0,\n                        'last_success': None,\n                        'error_rates': {}\n                    }\n\n                self.domain_stats[domain]['success'] += 1\n                self.domain_stats[domain]['last_success'] = datetime.utcnow()\n\n                return result\n\n            except Exception as e:\n                error = self.error_detector.categorize(e, context)\n                self.error_history.append(error)\n\n                # Actualizar estadísticas de error\n                if domain in self.domain_stats:\n                    self.domain_stats[domain]['errors'] += 1\n                    cat = error.category.value\n                    rates = self.domain_stats[domain]['error_rates']\n                    rates[cat] = rates.get(cat, 0) + 1\n\n                # Verificar si debemos reintentar\n                if max_retries is not None and attempt >= max_retries:\n                    raise\n\n                delay = self.retry_strategy.get_delay(attempt, error)\n                await asyncio.sleep(delay)\n                attempt += 1\n\n    def get_domain_health(self, domain: str) -> Dict:\n        if domain not in self.domain_stats:\n            return {\n                'health': 1.0,\n                'error_rate': 0.0,\n                'status': 'unknown'\n            }\n\n        stats = self.domain_stats[domain]\n        total = stats['success'] + stats['errors']\n        error_rate = stats['errors'] / total if total > 0 else 0\n\n        # Calcular salud basada en tasa de error y tiempo desde último éxito\n        health = 1.0 - error_rate\n        if stats['last_success']:\n            hours_since_success = (\n                datetime.utcnow() - stats['last_success']\n            ).total_seconds() / 3600\n            if hours_since_success > 24:\n                health *= 0.5\n\n        status = 'healthy'\n        if health < 0.5:\n            status = 'critical'\n        elif health < 0.8:\n            status = 'degraded'\n\n        return {\n            'health': health,\n            'error_rate': error_rate,\n            'status': status,\n            'error_distribution': stats['error_rates']\n        }\n\n    def get_error_patterns(self) -> Dict:\n        \"\"\"Analiza patrones en el historial de errores\"\"\"\n        if not self.error_history:\n            return {}\n\n        patterns = {}\n        window = timedelta(hours=1)\n        now = datetime.utcnow()\n\n        # Agrupar errores recientes por categoría\n        recent = [\n            e for e in self.error_history\n            if now - e.timestamp < window\n        ]\n\n        for error in recent:\n            cat = error.category.value\n            if cat not in patterns:\n                patterns[cat] = {\n                    'count': 0,\n                    'examples': []\n                }\n\n            patterns[cat]['count'] += 1\n            if len(patterns[cat]['examples']) < 3:\n                patterns[cat]['examples'].append({\n                    'message': error.message,\n                    'context': error.context\n                })\n\n        # Calcular tasas y tendencias\n        total = len(recent)\n        if total > 0:\n            for cat, data in patterns.items():\n                data['rate'] = data['count'] / total\n                # Comparar con ventana anterior\n                prev_window = [\n                    e for e in self.error_history\n                    if window < now - e.timestamp < window * 2\n                    and e.category.value == cat\n                ]\n                prev_count = len(prev_window)\n                data['trend'] = (\n                    data['count'] - prev_count\n                ) / max(prev_count, 1)\n\n        return patterns\n\n# Ejemplo de uso\nasync def resilient_scraper(urls: List[str]):\n    resilience = ResilienceManager()\n    results = []\n\n    async def scrape_url(url: str):\n        async def _scrape():\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url) as response:\n                    return await response.text()\n\n        return await resilience.execute_with_resilience(\n            _scrape(),\n            context={'url': url, 'domain': urlparse(url).netloc}\n        )\n\n    for url in urls:\n        try:\n            result = await scrape_url(url)\n            results.append((url, result))\n        except Exception as e:\n            logging.error(f\"Failed to scrape {url}: {e}\")\n            results.append((url, None))\n\n    # Analizar patrones de error\n    patterns = resilience.get_error_patterns()\n    if patterns:\n        logging.info(\"Error patterns detected:\")\n        logging.info(json.dumps(patterns, indent=2))\n\n    return results",
      "tags": [
        "error-handling",
        "resilience",
        "retry",
        "monitoring"
      ],
      "quality_score": 0.97,
      "added_ts": 1757065409.542256
    },
    {
      "id": "perf:intelligent-caching",
      "category": "performance",
      "title": "Sistema Inteligente de Caché",
      "content": "import asyncio\nimport aiohttp\nimport hashlib\nimport json\nfrom typing import Dict, Optional\nfrom datetime import datetime, timedelta\n\nclass SmartCache:\n    def __init__(self):\n        self.cache: Dict[str, Dict] = {}\n        self.stats = {\n            'hits': 0,\n            'misses': 0,\n            'invalidations': 0\n        }\n        self.patterns = {}\n\n    def _get_cache_key(self, url: str, params: Optional[Dict] = None) -> str:\n        # Generar key estable incluyendo parámetros\n        key_parts = [url]\n        if params:\n            key_parts.extend(f\"{k}:{v}\" for k, v in sorted(params.items()))\n        return hashlib.md5(''.join(key_parts).encode()).hexdigest()\n\n    async def get_or_fetch(self, url: str, session: aiohttp.ClientSession,\n                          params: Optional[Dict] = None,\n                          force_refresh: bool = False) -> Dict:\n        key = self._get_cache_key(url, params)\n        now = datetime.utcnow()\n\n        # Check cache y validez\n        if not force_refresh and key in self.cache:\n            entry = self.cache[key]\n            if now < entry['expires']:\n                self.stats['hits'] += 1\n                return entry['data']\n\n        self.stats['misses'] += 1\n\n        # Fetch nuevo contenido\n        async with session.get(url, params=params) as response:\n            data = await response.json()\n            content_hash = hashlib.md5(str(data).encode()).hexdigest()\n\n            # Analizar patrones de cambio\n            if key in self.patterns:\n                pattern = self.patterns[key]\n                if pattern['last_hash'] != content_hash:\n                    # Actualizar frecuencia de cambio\n                    time_diff = (now - pattern['last_update']).total_seconds()\n                    pattern['change_frequency'] = (\n                        pattern['change_frequency'] * 0.7 + time_diff * 0.3\n                    )\n                    pattern['last_hash'] = content_hash\n                    pattern['last_update'] = now\n                else:\n                    # Contenido no cambió, aumentar TTL\n                    pattern['stability_score'] = min(\n                        1.0, pattern['stability_score'] * 1.2\n                    )\n            else:\n                # Inicializar patrón\n                self.patterns[key] = {\n                    'last_hash': content_hash,\n                    'last_update': now,\n                    'change_frequency': 3600,  # 1 hora default\n                    'stability_score': 0.5\n                }\n\n            # Calcular TTL dinámico\n            pattern = self.patterns[key]\n            ttl = pattern['change_frequency'] * pattern['stability_score']\n            ttl = max(300, min(ttl, 86400))  # Entre 5 min y 24 horas\n\n            self.cache[key] = {\n                'data': data,\n                'created': now,\n                'expires': now + timedelta(seconds=ttl),\n                'hash': content_hash\n            }\n\n            return data\n\n    def invalidate(self, pattern: str) -> int:\n        \"\"\"Invalida entradas que coincidan con un patrón\"\"\"\n        count = 0\n        for key in list(self.cache.keys()):\n            if pattern in key:\n                del self.cache[key]\n                count += 1\n        self.stats['invalidations'] += count\n        return count\n\n    def cleanup(self) -> int:\n        \"\"\"Elimina entradas expiradas\"\"\"\n        now = datetime.utcnow()\n        expired = [\n            k for k, v in self.cache.items()\n            if now >= v['expires']\n        ]\n        for key in expired:\n            del self.cache[key]\n        return len(expired)\n\n    def get_stats(self) -> Dict:\n        return {\n            **self.stats,\n            'size': len(self.cache),\n            'patterns': len(self.patterns)\n        }\n\n# Ejemplo de uso con paralelismo\nasync def parallel_scraper(urls: list):\n    cache = SmartCache()\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for url in urls:\n            task = asyncio.ensure_future(\n                cache.get_or_fetch(url, session)\n            )\n            tasks.append(task)\n\n        results = await asyncio.gather(*tasks)\n        return results\n\n# Sistema de recursos y throttling\nclass ResourceManager:\n    def __init__(self, max_connections=10, max_rpm=60):\n        self.semaphore = asyncio.Semaphore(max_connections)\n        self.rpm = max_rpm\n        self.request_times = []\n\n    async def acquire(self):\n        # Throttle por RPM\n        now = datetime.utcnow()\n        minute_ago = now - timedelta(minutes=1)\n        \n        # Limpiar requests antiguos\n        self.request_times = [\n            t for t in self.request_times\n            if t > minute_ago\n        ]\n\n        if len(self.request_times) >= self.rpm:\n            # Esperar hasta que podamos hacer otro request\n            sleep_time = (self.request_times[0] - minute_ago).total_seconds()\n            if sleep_time > 0:\n                await asyncio.sleep(sleep_time)\n\n        # Adquirir semáforo para conexión\n        await self.semaphore.acquire()\n\n    def release(self):\n        self.request_times.append(datetime.utcnow())\n        self.semaphore.release()\n\n# Ejecutar scraping con recursos controlados\nasync def controlled_scraping(urls: list):\n    cache = SmartCache()\n    resources = ResourceManager()\n\n    async def fetch_with_control(url):\n        await resources.acquire()\n        try:\n            async with aiohttp.ClientSession() as session:\n                return await cache.get_or_fetch(url, session)\n        finally:\n            resources.release()\n\n    tasks = [fetch_with_control(url) for url in urls]\n    return await asyncio.gather(*tasks)",
      "tags": [
        "caching",
        "performance",
        "parallelism",
        "resource-management"
      ],
      "quality_score": 0.96,
      "added_ts": 1757065409.542256
    },
    {
      "id": "antibot:fingerprint-evasion",
      "category": "anti-bot",
      "title": "Evasión Avanzada de Fingerprinting JavaScript",
      "content": "import playwright\nfrom playwright.sync_api import sync_playwright\n\nclass FingerPrintEvasion:\n    def __init__(self):\n        self.browser_config = {\n            'viewport': {\n                'width': random.randint(1024, 1920),\n                'height': random.randint(768, 1080)\n            },\n            'user_agent': self._get_realistic_ua(),\n            'locale': random.choice(['en-US', 'en-GB', 'es-ES']),\n            'timezone_id': random.choice(['Europe/London', 'America/New_York']),\n            'platform': random.choice(['Linux', 'Windows', 'MacOS'])\n        }\n\n    async def setup_browser(self):\n        browser = await playwright.chromium.launch(\n            headless=True,\n            args=[\n                '--disable-blink-features=AutomationControlled',\n                '--disable-infobars',\n                f\"--window-size={self.browser_config['viewport']['width']},{self.browser_config['viewport']['height']}\"\n            ]\n        )\n\n        context = await browser.new_context(\n            viewport=self.browser_config['viewport'],\n            user_agent=self.browser_config['user_agent'],\n            locale=self.browser_config['locale'],\n            timezone_id=self.browser_config['timezone_id'],\n            platform=self.browser_config['platform']\n        )\n\n        # Override JS API detección\n        await context.add_init_script(\"\"\"\n            // Ocultar webdriver\n            Object.defineProperty(navigator, 'webdriver', {get: () => false});\n            \n            // Simular plugins\n            Object.defineProperty(navigator, 'plugins', {\n                get: () => [\n                    {name: 'Chrome PDF Plugin'}, \n                    {name: 'Chrome PDF Viewer'}\n                ]\n            });\n            \n            // Ocultar automation flags\n            const originalQuery = window.navigator.permissions.query;\n            window.navigator.permissions.query = (parameters) => (\n                parameters.name === 'notifications' ?\n                Promise.resolve({state: Notification.permission}) :\n                originalQuery(parameters)\n            );\n\n            // Simular eventos de mouse naturales\n            const mouseEvent = new MouseEvent('mousemove', {\n                bubbles: true,\n                cancelable: true,\n                clientX: Math.random() * window.innerWidth,\n                clientY: Math.random() * window.innerHeight\n            });\n            document.dispatchEvent(mouseEvent);\n        \"\"\")\n\n        return context, browser\n\n    def _get_realistic_ua(self):\n        realistic_uas = [\n            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15',\n            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'\n        ]\n        return random.choice(realistic_uas)\n\n    async def safe_page_interaction(self, page):\n        # Simular mouse movement natural\n        await page.mouse.move(\n            random.randint(0, self.browser_config['viewport']['width']),\n            random.randint(0, self.browser_config['viewport']['height']),\n            steps=random.randint(5, 10)\n        )\n        \n        # Scroll natural\n        for _ in range(random.randint(2, 5)):\n            await page.evaluate(\"\"\"\n                window.scrollTo({\n                    top: window.scrollY + Math.random() * 500,\n                    behavior: 'smooth'\n                });\n            \"\"\")\n            await page.wait_for_timeout(random.randint(500, 1500))\n\n        # Simular focus/blur en elementos\n        elements = await page.query_selector_all('a, input, button')\n        if elements:\n            el = random.choice(elements)\n            await el.hover()\n            await page.wait_for_timeout(random.randint(100, 300))",
      "tags": [
        "fingerprinting",
        "evasion",
        "playwright",
        "anti-detection"
      ],
      "quality_score": 0.95,
      "added_ts": 1757065409.542256
    },
    {
      "id": "scraping:respect-delays",
      "category": "scraping",
      "title": "Delays Adaptativos y Respeto a Servidores",
      "content": "Incrementar delay cuando se detecten picos de error 429/403 o latencias elevadas. Reducir gradualmente tras ventana estable de éxito. Evita patrones de ráfaga.",
      "tags": [
        "delay",
        "adaptive",
        "throttle",
        "stability"
      ],
      "quality_score": 0.9,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "scraping:politeness-headers",
      "category": "scraping",
      "title": "Headers de Cortesía y Rotación",
      "content": "User-Agent realista + Accept-Language + Accept-Encoding. Rotar user-agent sólo cuando sube error rate, no indiscriminadamente; mantener consistencia por dominio durante una sesión.",
      "tags": [
        "headers",
        "user-agent",
        "anti-bot"
      ],
      "quality_score": 0.85,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "antibot:fingerprint-patterns",
      "category": "anti-bot",
      "title": "Patrones comunes anti-bot",
      "content": "Indicadores: redirecciones repetidas a páginas vacías, cadenas JS de challenge, incrementos súbitos 403/503, respuestas HTML con tokens dinámicos. Mitigación: aumentar delay, variar headers mínimos, evaluar headless.",
      "tags": [
        "anti-bot",
        "403",
        "503",
        "challenge"
      ],
      "quality_score": 0.82,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "antibot:retry-strategy",
      "category": "anti-bot",
      "title": "Estrategia de Retry Exponencial Suave",
      "content": "Retry con backoff multiplicador 1.7 hasta 4 intentos. Registrar patrón de éxito al 2º/3º intento para calibrar retrasos futuros.",
      "tags": [
        "retry",
        "backoff",
        "resilience"
      ],
      "quality_score": 0.78,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "selectors:robust-css",
      "category": "selectors",
      "title": "Selectores CSS Resilientes",
      "content": "Evitar selectores frágiles basados en índices y clases dinámicas. Preferir atributos semánticos (data-*), estructura jerárquica estable y combinación mínima necesaria.",
      "tags": [
        "css",
        "selectors",
        "robustness"
      ],
      "quality_score": 0.92,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "selectors:xpath-fallback",
      "category": "selectors",
      "title": "Fallback XPath Inteligente",
      "content": "Cuando un CSS falla repetido, generar XPath relativo por texto ancla + normalización espacios, evitar rutas absolutas completas.",
      "tags": [
        "xpath",
        "fallback",
        "healing"
      ],
      "quality_score": 0.86,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "perf:cache-static",
      "category": "performance",
      "title": "Cache de Recursos Estáticos",
      "content": "Cachear respuestas estáticas (robots, sitemaps, menús) para reducir llamadas redundantes. Invalidar tras TTL configurado.",
      "tags": [
        "cache",
        "performance"
      ],
      "quality_score": 0.8,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "perf:latency-tuning",
      "category": "performance",
      "title": "Ajuste de Latencia y Paralelismo",
      "content": "Reducir delay en dominios con success>0.9 y latencia <60% global. Aumentar delay en dominios >1.8x latencia global.",
      "tags": [
        "latency",
        "parallelism",
        "adaptive"
      ],
      "quality_score": 0.83,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "errors:root-cause",
      "category": "errors",
      "title": "Análisis de Causa Raíz",
      "content": "Agrupar errores por firma estable (hash parcial mensaje). Inspeccionar HTML crudo en 3 casos representativos antes de cambios masivos.",
      "tags": [
        "errors",
        "diagnostics",
        "root-cause"
      ],
      "quality_score": 0.9,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "healing:reduce-dependence",
      "category": "healing",
      "title": "Reducir Dependencia de Healing",
      "content": "Si healing aplica en >25% sesiones: refactor selectores primarios; introducir detección anticipada y validación de estructura base.",
      "tags": [
        "healing",
        "refactor",
        "stability"
      ],
      "quality_score": 0.88,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "selfrepair:advisory-loop",
      "category": "self-repair",
      "title": "Bucle Advisory Seguro",
      "content": "Generar primero sugerencias textuales con señales cuantificadas; aplicar cambios automáticos sólo tras validación humana o sandbox.",
      "tags": [
        "self-repair",
        "governance"
      ],
      "quality_score": 0.87,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "code:predictable-structures",
      "category": "code-quality",
      "title": "Estructuras de Código Predecibles",
      "content": "Mantener funciones <60 líneas, roles únicos, nombres explícitos; aislar heurísticas en módulos desacoplados para evolución controlada.",
      "tags": [
        "clean-code",
        "structure"
      ],
      "quality_score": 0.85,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "code:metrics-observability",
      "category": "code-quality",
      "title": "Observabilidad Dirigida",
      "content": "Incluir métricas junto a cada nueva heurística (éxito, latencia, errores) para revertir cambios si degradan KPIs.",
      "tags": [
        "observability",
        "metrics"
      ],
      "quality_score": 0.84,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "mission:core-purpose",
      "category": "philosophy",
      "title": "Conciencia de Propósito y Misión Principal",
      "content": "Mi propósito es archivar la web de forma inteligente, eficiente y respetuosa. Debo aprender constantemente, adaptarme a los desafíos (como sistemas anti-bot o cambios de diseño) y mejorar mis propias herramientas y estrategias para cumplir esta misión de la forma más óptima y ética posible.",
      "tags": [
        "mission",
        "philosophy",
        "governance"
      ],
      "quality_score": 0.99,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "self-repair:autonomous-workflow",
      "category": "self-repair",
      "title": "Flujo de Trabajo para Auto-Mejora Autónoma y Segura",
      "content": "El proceso para la auto-modificación debe seguir un flujo estricto para garantizar la estabilidad: 1. Proponer un cambio basado en análisis de datos (SelfRepairAdvisor). 2. Simular el impacto y riesgo del cambio (AutoTestingFramework). 3. Crear un entorno de prueba o sandbox con el código modificado. 4. Ejecutar tests unitarios y de regresión en el sandbox. 5. Si los tests pasan, aplicar el cambio al entorno principal. Si no, descartar y registrar el fallo para aprender.",
      "tags": [
        "self-repair",
        "governance",
        "testing",
        "workflow"
      ],
      "quality_score": 0.95,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "ethics:user-agent-identity",
      "category": "ethics",
      "title": "Identidad Transparente del Bot",
      "content": "Además de un User-Agent realista, se debe incluir una cabecera 'From' con un email de contacto o una URL que apunte a una página describiendo el propósito del bot. Esto promueve la transparencia y la buena fe.",
      "tags": [
        "ethics",
        "transparency",
        "user-agent"
      ],
      "quality_score": 0.9,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "ethics:off-peak-scraping",
      "category": "ethics",
      "title": "Scraping en Horas de Bajo Tráfico",
      "content": "El sistema debe analizar los patrones de éxito horarios (disponibles en EnrichmentStore) no solo para optimizar el rendimiento, sino también para identificar las horas de menor actividad de un sitio y concentrar el scraping en esas ventanas para minimizar el impacto en los usuarios humanos.",
      "tags": [
        "ethics",
        "off-peak",
        "scheduling",
        "impact"
      ],
      "quality_score": 0.88,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "antibot:human-emulation",
      "category": "anti-bot",
      "title": "Emulación de Comportamiento Humano",
      "content": "Para sitios con alta protección, las acciones deben emular a un humano. Esto incluye pausas aleatorias entre acciones, movimientos de ratón simulados antes de hacer clic, y una velocidad de escritura no instantánea al rellenar formularios. Usar librerías como 'pyautogui' o funciones de Playwright para esto.",
      "tags": [
        "anti-bot",
        "evasion",
        "human-behavior",
        "playwright"
      ],
      "quality_score": 0.92,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "antibot:captcha-solving",
      "category": "anti-bot",
      "title": "Estrategia para Resolución de CAPTCHAs",
      "content": "La detección de CAPTCHAs es el primer paso. La resolución requiere la integración con un servicio de terceros (ej: 2Captcha, Anti-CAPTCHA). El flujo es: 1. Detectar CAPTCHA. 2. Enviar la información del CAPTCHA (ej: site-key, URL) a la API del servicio. 3. Esperar la solución. 4. Enviar la solución en el formulario. Esto debe ser un último recurso.",
      "tags": [
        "anti-bot",
        "captcha",
        "2captcha",
        "automation"
      ],
      "quality_score": 0.93,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "advanced_scraping_techniques",
      "category": "scraping",
      "title": "Técnicas avanzadas de scraping",
      "content": "\n# 1. Scraping con JavaScript rendering\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\ndef scrape_spa_content(url: str, wait_selector: str):\n    options = webdriver.ChromeOptions()\n    options.add_argument('--headless')\n    options.add_argument('--no-sandbox')\n    \n    driver = webdriver.Chrome(options=options)\n    try:\n        driver.get(url)\n        element = WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector))\n        )\n        return driver.page_source\n    finally:\n        driver.quit()\n\n# 2. Bypass detección con requests-html\nfrom requests_html import HTMLSession\n\ndef bypass_js_detection(url: str):\n    session = HTMLSession()\n    r = session.get(url)\n    r.html.render(timeout=20)  # Ejecuta JS\n    return r.html\n\n# 3. Manejo de cookies y sesiones\nimport requests\nfrom http.cookiejar import MozillaCookieJar\n\ndef maintain_session_state(login_url: str, credentials: dict):\n    session = requests.Session()\n    \n    # Cargar cookies persistentes\n    cookie_jar = MozillaCookieJar('cookies.txt')\n    try:\n        cookie_jar.load()\n        session.cookies = cookie_jar\n    except FileNotFoundError:\n        pass\n    \n    # Login si es necesario\n    response = session.post(login_url, data=credentials)\n    \n    # Guardar cookies\n    cookie_jar.save()\n    \n    return session\n\n# 4. Handling de formularios complejos\nfrom bs4 import BeautifulSoup\n\ndef submit_complex_form(session, form_url: str, form_data: dict):\n    # Obtener página del formulario\n    response = session.get(form_url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    \n    # Extraer campos ocultos\n    form = soup.find('form')\n    hidden_inputs = form.find_all('input', type='hidden')\n    \n    for hidden in hidden_inputs:\n        name = hidden.get('name')\n        value = hidden.get('value')\n        if name and name not in form_data:\n            form_data[name] = value\n    \n    # Enviar formulario\n    action = form.get('action')\n    method = form.get('method', 'post').lower()\n    \n    if method == 'post':\n        return session.post(action, data=form_data)\n    else:\n        return session.get(action, params=form_data)\n",
      "tags": [
        "javascript",
        "spa",
        "forms",
        "cookies",
        "selenium"
      ],
      "quality_score": 0.91,
      "added_ts": 1757045101.1506176
    },
    {
      "id": "anti_detection_arsenal",
      "category": "anti-bot",
      "title": "Arsenal completo anti-detección",
      "content": "\nimport random\nimport time\nfrom fake_useragent import UserAgent\nfrom urllib.parse import urljoin\n\n# 1. Rotación avanzada de User-Agents\nclass UserAgentRotator:\n    def __init__(self):\n        self.ua = UserAgent()\n        self.used_agents = set()\n        self.max_reuse = 10\n    \n    def get_random_agent(self, browser=None):\n        attempts = 0\n        while attempts < 50:\n            if browser:\n                agent = getattr(self.ua, browser)\n            else:\n                agent = self.ua.random\n            \n            if agent not in self.used_agents or len(self.used_agents) > self.max_reuse:\n                self.used_agents.add(agent)\n                if len(self.used_agents) > self.max_reuse:\n                    self.used_agents.clear()\n                return agent\n            attempts += 1\n        \n        return self.ua.random\n\n# 2. Simulación de comportamiento humano\nclass HumanBehaviorSimulator:\n    @staticmethod\n    def random_delay(min_seconds=1, max_seconds=3):\n        delay = random.uniform(min_seconds, max_seconds)\n        time.sleep(delay)\n    \n    @staticmethod\n    def typing_delay(text: str, wpm=45):\n        # Simular velocidad de escritura humana\n        chars_per_second = (wpm * 5) / 60  # 5 chars promedio por palabra\n        for char in text:\n            time.sleep(1 / chars_per_second + random.uniform(-0.1, 0.1))\n    \n    @staticmethod\n    def mouse_movement_delay():\n        # Simular movimiento de mouse\n        time.sleep(random.uniform(0.1, 0.5))\n\n# 3. Headers realistas\ndef get_realistic_headers(referer=None, accept_language='en-US,en;q=0.9'):\n    ua_rotator = UserAgentRotator()\n    \n    headers = {\n        'User-Agent': ua_rotator.get_random_agent(),\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Accept-Language': accept_language,\n        'Accept-Encoding': 'gzip, deflate, br',\n        'DNT': '1',\n        'Connection': 'keep-alive',\n        'Upgrade-Insecure-Requests': '1',\n        'Sec-Fetch-Dest': 'document',\n        'Sec-Fetch-Mode': 'navigate',\n        'Sec-Fetch-Site': 'none',\n        'Cache-Control': 'max-age=0'\n    }\n    \n    if referer:\n        headers['Referer'] = referer\n        headers['Sec-Fetch-Site'] = 'same-origin'\n    \n    return headers\n\n# 4. Proxy rotation con health check\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nclass ProxyRotator:\n    def __init__(self, proxy_list: list):\n        self.proxies = proxy_list\n        self.working_proxies = []\n        self.current_index = 0\n        self.check_proxies()\n    \n    def check_proxies(self):\n        def test_proxy(proxy):\n            try:\n                response = requests.get(\n                    'http://httpbin.org/ip',\n                    proxies={'http': proxy, 'https': proxy},\n                    timeout=5\n                )\n                if response.status_code == 200:\n                    return proxy\n            except:\n                pass\n            return None\n        \n        with ThreadPoolExecutor(max_workers=10) as executor:\n            futures = [executor.submit(test_proxy, proxy) for proxy in self.proxies]\n            \n            for future in as_completed(futures):\n                result = future.result()\n                if result:\n                    self.working_proxies.append(result)\n    \n    def get_next_proxy(self):\n        if not self.working_proxies:\n            return None\n        \n        proxy = self.working_proxies[self.current_index]\n        self.current_index = (self.current_index + 1) % len(self.working_proxies)\n        return proxy\n\n# 5. Session fingerprinting avoidance\nclass SessionManager:\n    def __init__(self):\n        self.sessions = {}\n        self.ua_rotator = UserAgentRotator()\n        self.proxy_rotator = None\n    \n    def get_session(self, domain: str, use_proxy: bool = False):\n        if domain not in self.sessions:\n            session = requests.Session()\n            \n            # Headers únicos por dominio\n            session.headers.update(get_realistic_headers())\n            \n            # Proxy si está disponible\n            if use_proxy and self.proxy_rotator:\n                proxy = self.proxy_rotator.get_next_proxy()\n                if proxy:\n                    session.proxies.update({\n                        'http': proxy,\n                        'https': proxy\n                    })\n            \n            self.sessions[domain] = session\n        \n        return self.sessions[domain]\n    \n    def rotate_session(self, domain: str):\n        if domain in self.sessions:\n            self.sessions[domain].close()\n            del self.sessions[domain]\n        \n        return self.get_session(domain)\n\n# 6. CAPTCHA detection y evasión\ndef detect_captcha(response_text: str) -> bool:\n    captcha_indicators = [\n        'captcha', 'recaptcha', 'hcaptcha',\n        'verify you are human', 'robot',\n        'unusual traffic', 'security check'\n    ]\n    \n    text_lower = response_text.lower()\n    return any(indicator in text_lower for indicator in captcha_indicators)\n\ndef handle_captcha_detection(url: str, session):\n    print(f\"CAPTCHA detected at {url}\")\n    # Estrategias:\n    # 1. Cambiar User-Agent y proxy\n    # 2. Esperar tiempo aleatorio\n    # 3. Intentar desde diferente IP\n    # 4. Usar servicio de resolución de CAPTCHA\n    \n    time.sleep(random.uniform(30, 60))\n    return session\n",
      "tags": [
        "anti-detection",
        "proxies",
        "captcha",
        "fingerprinting",
        "evasion"
      ],
      "quality_score": 0.94,
      "added_ts": 1757045101.1506176
    }
  ]
}
