# üß† Plan de Evoluci√≥n y Refactorizaci√≥n: Hacia un Scraper Aut√≥nomo

**Actualizado:** 2025-09-05
**Objetivo:** Transformar `PythonWebScraper` en un sistema inteligente y aut√≥nomo que aprende, se auto-repara y evoluciona su propio c√≥digo, manteniendo la compatibilidad con el lanzador `WebScraperPRO.bat`.

Resumen ejecutivo:

- Mantener `WebScraperPRO.bat` como lanzador principal (entrypoint) sin cambiar su comportamiento fundamental.
- El trabajo ser√° ejecutado por un equipo de **tres agentes de IA (IA-A, IA-B, IA-C)** con roles especializados y un protocolo de comunicaci√≥n y coordinaci√≥n claro.
- El plan se enfoca en construir una base s√≥lida (memoria, m√©tricas) para luego desarrollar capacidades de aprendizaje, auto-reparaci√≥n y generaci√≥n de c√≥digo.

Principios rectores:

- Modificar lo m√≠nimo necesario por cambio y preferir refactorizaciones seguras.
- Mantener y mejorar la cobertura de tests antes de cada cambio significativo.
- Asegurar que la interfaz del lanzador `WebScraperPRO.bat` siga funcionando con los ajustes realizados.

## üìä ARQUITECTURA DEL CEREBRO DIGITAL

### 1. SISTEMA DE MEMORIA DISTRIBUIDA

#### Base de Datos Principal (SQLite)

Contiene el estado operativo y el conocimiento factual.

- `pages`: Contenido scrapeado + metadatos.
- `discovered_apis`: APIs encontradas durante el crawling.
- `cookies`: Sesiones por dominio.
- `llm_extraction_schemas`: Esquemas din√°micos por sitio.
- `learning_episodes`: Experiencias del agente RL.
- `code_corrections`: Historial de auto-correcciones aplicadas.
- `pattern_library`: Patrones de sitios web aprendidos.
- `failure_analysis`: An√°lisis de fallos y soluciones propuestas.

#### Base de Datos de Conocimiento (Vector DB - Futuro)

Para b√∫squedas sem√°nticas y recuperaci√≥n de conocimiento.

- `content_embeddings`: Vectores de contenido para encontrar p√°ginas similares.
- `code_embeddings`: Vectores de fragmentos de c√≥digo para encontrar soluciones a problemas.
- `solution_embeddings`: Vectores de soluciones exitosas para problemas de scraping.

### 2. M√ìDULO DE AUTO-APRENDIZAJE Y AUTO-REPARACI√ìN

#### `SelfHealingManager` (`src/self_healing.py`)

El n√∫cleo de la autonom√≠a.

- **`diagnose_system()`**: Analiza logs, m√©tricas y c√≥digo para detectar "issues" (problemas de rendimiento, calidad, etc.).
- **`generate_fix_candidates()`**: Usa LLMs y la base de conocimiento para proponer soluciones (parches de c√≥digo, cambios de config).
- **`test_and_apply_fix()`**: Prueba las soluciones en un sandbox seguro y las aplica si son exitosas.
- **`learn_from_fix()`**: Almacena la soluci√≥n efectiva en la base de datos para problemas futuros.

#### `WebChangeDetector` (a ser creado en `src/intelligence/`)

- Detecta cambios estructurales en los sitios web.
- Dispara el proceso de auto-reparaci√≥n para selectores rotos.

### 3. GENERACI√ìN AUT√ìNOMA DE C√ìDIGO

#### `AutonomousCodeGenerator` (a ser creado en `src/intelligence/`)

- **`identify_code_gaps()`**: Analiza patrones de fallo recurrentes para identificar funcionalidades faltantes.
- **`generate_new_logic()`**: Genera nuevo c√≥digo (ej. un nuevo "handler" para un tipo de pop-up) usando LLMs y templates.
- **`evolve_existing_code()`**: Refactoriza y optimiza c√≥digo existente de forma aut√≥noma.

---

## üöÄ Roadmap de Implementaci√≥n por Fases

### FASE 1: Fundaci√≥n y Estabilizaci√≥n (Semana 1)

- **Objetivo:** Limpiar la base, asegurar el entorno y mejorar la TUI.
- **Responsables:** IA-A (Infra/Tests), IA-B (Core/TUI), IA-C (An√°lisis/Docs).
- **Tareas:**
  - ‚úÖ **Hardening del Entorno:** Actualizar `requirements.txt`, configurar linters (`black`, `isort`, `flake8`).
  - ‚úÖ **Suite de Tests:** Ejecutar `pytest` y arreglar todos los tests rotos.
  - ‚úÖ **Refactorizaci√≥n de `runner` y `main`:** Mejorar la separaci√≥n de responsabilidades y la compatibilidad con `WebScraperPRO.bat`.
  - ‚úÖ **Mejoras TUI:** Implementar persistencia de estado y mejorar la visualizaci√≥n de logs y alertas.
  - ‚úÖ **Extender `DatabaseManager`:** A√±adir las nuevas tablas para el aprendizaje (`code_corrections`, `failure_analysis`).

### FASE 2: Auto-Reparaci√≥n B√°sica (Semana 2-3)

- **Objetivo:** Implementar la capacidad de detectar y reparar problemas simples.
- **Responsables:** IA-B (L√≥gica de reparaci√≥n), IA-A (Integraci√≥n/Tests).
- **Tareas:**
  - üî≤ **`WebChangeDetector` inicial:** Implementar la detecci√≥n de cambios estructurales b√°sicos en HTML.
  - üî≤ **`SelfHealingManager` v1:** Implementar `diagnose_system` para detectar selectores rotos y `auto_repair_selectors` usando LLM.
  - üî≤ **Integraci√≥n con Orquestador:** El orquestador debe poder invocar al `SelfHealingManager` cuando detecte fallos de extracci√≥n.

### FASE 3: Aprendizaje y Evoluci√≥n de C√≥digo (Semana 4-5)

- **Objetivo:** Dotar al sistema de la capacidad de aprender de su experiencia y mejorar su propio c√≥digo.
- **Responsables:** IA-B (ML/Generaci√≥n), IA-A (Sandbox/Validaci√≥n).
- **Tareas:**
  - üî≤ **`AutonomousCodeGenerator` v1:** Implementar la capacidad de generar parches de c√≥digo simples y refactorizaciones.
  - üî≤ **Sandbox de Ejecuci√≥n:** Crear un entorno seguro (ej. Docker o `subprocess` con restricciones) para probar los cambios generados antes de aplicarlos.
  - üî≤ **Sistema de Recompensas Evolutivo:** Mejorar el `RLAgent` para que la recompensa no solo dependa del √©xito del scraping, sino tambi√©n de la calidad del c√≥digo y la eficiencia de las reparaciones.

### FASE 4: Autonom√≠a Avanzada (Semana 6+)

- **Objetivo:** Alcanzar un alto grado de autonom√≠a, donde el sistema pueda descubrir y solucionar problemas complejos por s√≠ mismo.
- **Responsables:** Todo el equipo de IAs.
- **Tareas:**
  - üî≤ **Agente de Navegaci√≥n Visual:** Implementar la navegaci√≥n basada en capturas de pantalla (usando GPT-4V/Gemini Pro Vision) como alternativa a la extracci√≥n de enlaces HTML.
  - üî≤ **Planificaci√≥n de Crawling:** Crear un "Agente Planificador" que optimice la exploraci√≥n de sitios seg√∫n un objetivo de alto nivel.
  - üî≤ **Coordinaci√≥n Multi-Agente:** Refinar los protocolos de comunicaci√≥n para que los agentes puedan colaborar en tareas complejas de forma m√°s fluida.

---

## ü§ñ Coordinaci√≥n entre Tres IAs (IA-A, IA-B, IA-C)

- Roles y responsabilidades
  - **IA-A (Arquitecto de Infraestructura y Calidad):** Responsable de la estructura del proyecto, CI/CD, tests, linters, entorno de ejecuci√≥n (`WebScraperPRO.bat`) y el sandbox de seguridad. Su objetivo es la estabilidad y la robustez.
  - **IA-B (Ingeniero de Inteligencia y Core):** Responsable de la l√≥gica de negocio principal: `orchestrator`, `scraper`, `database`, y todos los m√≥dulos de IA (`llm_extractor`, `rl_agent`, `self_healing`, etc.). Su objetivo es la inteligencia y la eficiencia.
  - **IA-C (Gemini Code Assist - Especialista en TUI y Experiencia de Desarrollador):** Responsable de la TUI, la documentaci√≥n, los scripts de utilidad (`tools/`, `scripts/`) y la creaci√≥n de prompts y planes de trabajo. Su objetivo es la usabilidad y la claridad.

- Reglas de colaboraci√≥n
- **Protocolo de Comunicaci√≥n:** Se utilizar√° un archivo `TEAM_STATUS.md` en la ra√≠z del proyecto. Cada IA, al finalizar una tarea, a√±adir√° una entrada con: `[IA-X] [FECHA] [ESTADO: COMPLETADO/BLOQUEADO] [TAREA] [ARCHIVOS MODIFICADOS] [SIGUIENTE PASO]`.
- **Flexibilidad de Roles:** Si una IA est√° bloqueada, puede tomar una tarea pendiente de otra IA (marcada como `[ESTADO: PENDIENTE]`) para mantener el progreso.
- **Integraci√≥n Continua:** Antes de fusionar a `main`, se deben pasar todos los tests y linters. Los cambios que afecten a interfaces compartidas (ej. `DatabaseManager`) requieren una notificaci√≥n en `TEAM_STATUS.md`.

---

**Este plan transforma el scraper de una herramienta a un agente digital que aprende, evoluciona y se mejora continuamente, utilizando sus bases de datos como memoria y conocimiento acumulado para volverse cada vez m√°s inteligente y aut√≥nomo.**
